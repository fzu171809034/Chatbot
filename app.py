from langchain_openai import ChatOpenAI
from langchain.chains import ConversationalRetrievalChain
from memory.memory import get_memory
from retrieval.rag_engine import load_corp_files, load_file_to_vectorstore  # ä¿®æ”¹ç‚¹
from database.storage import save_message, load_history
import gradio as gr
import os
from config import OPENAI_API_KEY, DB_PATH
from langchain_core.messages import AIMessage

os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

llm = ChatOpenAI(temperature=0.7, model_name="gpt-3.5-turbo")
memory = get_memory()

retriever = None
qa_chain = None
uploaded_files = []


def update_vectorstore():
    global retriever, qa_chain

    print("[INFO] åŠ è½½ä¼ä¸šçŸ¥è¯†åº“...")
    base_vs = load_corp_files()

    docs = [base_vs]
    for f in uploaded_files:
        print(f"[INFO] åŠ è½½ç”¨æˆ·æ–‡ä»¶: {f.name}")
        file_vs = load_file_to_vectorstore(f.name)
        docs.append(file_vs)

    merged_vs = docs[0]
    for other in docs[1:]:
        merged_vs.merge_from(other)

    retriever = merged_vs.as_retriever()
    qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)


update_vectorstore()


def chatbot_interface(message, file):
    global uploaded_files

    if file is not None:
        if not isinstance(file, list):
            file = [file]
        uploaded_files.extend(file)
        update_vectorstore()
        return f"ğŸ“ å…±åŠ è½½ {len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼ˆå«ä¼ä¸šçŸ¥è¯†åº“ï¼‰"

    if qa_chain:
        result = qa_chain.invoke({"question": message})
        answer = result["answer"]
    else:
        result = llm.invoke(message)
        answer = result.content if isinstance(result, AIMessage) else str(result)

    save_message(message, answer)
    return answer


chat_history = []

with gr.Blocks(css=".chatbox {height: 500px; overflow-y: auto;}") as demo:
    gr.Markdown("## ğŸ¤– ä¼ä¸šçŸ¥è¯†åº“é—®ç­” + æ–‡ä»¶ Q&A")

    with gr.Row():
        with gr.Column(scale=2):
            history_output = gr.Textbox(label="ğŸ“œ æœ€è¿‘å†å²è®°å½•", lines=25, interactive=False)
            history_btn = gr.Button("ğŸ•˜ æŸ¥çœ‹å†å²è®°å½•", scale=1)

        with gr.Column(scale=5):
            chatbot = gr.Chatbot(label="å¯¹è¯è®°å½•", elem_classes="chatbox", type="messages")
            with gr.Row():
                file_input = gr.File(file_types=[".pdf", ".txt"], file_count="multiple", label="ğŸ“ ä¸Šä¼ æ–‡ä»¶")
            with gr.Row():
                message = gr.Textbox(placeholder="è¾“å…¥ä½ çš„é—®é¢˜...", lines=1, scale=8, show_label=False)
                submit_btn = gr.Button("ğŸš€ å‘é€")

    def chat_handler(msg, files):
        global chat_history
        file_msg = ""

        if files is not None:
            result = chatbot_interface("", files)
            file_msg = result

        if msg.strip() != "":
            result = chatbot_interface(msg, None)
            chat_history.append({"role": "user", "content": msg})
            chat_history.append({"role": "assistant", "content": result})
            return "", chat_history

        if file_msg:
            chat_history.append({"role": "user", "content": "ğŸ“ ä¸Šä¼ æ–‡ä»¶"})
            chat_history.append({"role": "assistant", "content": file_msg})
            return "", chat_history

        return "", chat_history

    submit_btn.click(fn=chat_handler, inputs=[message, file_input], outputs=[message, chatbot])
    history_btn.click(fn=load_history, outputs=history_output)

if __name__ == "__main__":
    demo.launch()
